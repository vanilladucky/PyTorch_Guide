{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size:50px;\">About RNN</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size:30px;\">Theory</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN is a popular form of neural network, mainly used for language tasks, sequential problems such as time series. \n",
    "#### It stands for Recurent Neural Network.\n",
    "#### Unlike a conventional Deep Neural Network which showcases layers of nodes that are connected by weights which will take a certain set of input and output certain output(s), RNN specializes in sequential data. \n",
    "#### It utilizes hidden states, calculated after every inputs to 'remember' or 'store' information along the way. \n",
    "#### In theory, this is important for sequential task such as language modeling as human conversation takes into account past words with varying degree of importance. \n",
    "#### Of course RNN is the simplest form and architecture there is to this field of networks and there are more advanced ones such as LSTUs and GRUs but I will only talk about RNN in this notebook.\n",
    "#### The essential formula for a RNN is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\LARGE h_{t+1} = f(w_x \\cdot x_t + w_h \\cdot h_t + b_h) $$\n",
    "$$\\LARGE y_t = f(w_y \\cdot h_t + b_y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basically, $h$ represents the hidden state within the RNN. \n",
    "#### In RNNs, hidden states are the outputs and the inputs.\n",
    "#### So in one cycle, the previous hidden state ($h_t$) and input at that time ($x_t$) will be put in. \n",
    "#### It would output a hidden state ($h_{t+1}$) which would be fed into the next cycle and also could be used to calculate the output in a conventional forward neural network. \n",
    "#### One thing to keep in mind is that the weights and biases used is the **same throughout** all of these cycles. \n",
    "#### The training and backpropagation is responsible for finding the optimal weights and biases numerical figures for the inputs and outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size:30px;\">Code Implementation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "import torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For simplicity, we will come up with simple data we make ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey how are you', 'good i am fine', 'have a nice day']\n",
    "\n",
    "chars = set(''.join(text)) # will create a set of unique characters in the text array above\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### int2char method will make a dictionary with the index as key and characters as value \n",
    "#### char2int method will create a dictionary that has characters as key and the respective indices in int2char as values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what int2char looks like {0: 'u', 1: ' ', 2: 'r', 3: 'd', 4: 'y', 5: 'g', 6: 'v', 7: 'c', 8: 'o', 9: 'f', 10: 'e', 11: 'n', 12: 'h', 13: 'i', 14: 'm', 15: 'w', 16: 'a'}\n",
      "\n",
      "This is what char2int looks like {'u': 0, ' ': 1, 'r': 2, 'd': 3, 'y': 4, 'g': 5, 'v': 6, 'c': 7, 'o': 8, 'f': 9, 'e': 10, 'n': 11, 'h': 12, 'i': 13, 'm': 14, 'w': 15, 'a': 16}\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is what int2char looks like {int2char}\\n\") \n",
    "print(f\"This is what char2int looks like {char2int}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will also apply **padding** to our data. \n",
    "#### While RNNs are typically able to take in variably sized inputs, we usually want to feed the data in batches and thus need to ensure they are the same size. \n",
    "#### For sentences too short, we fill them up with 0 values and trim those that are too long. \n",
    "#### For our case, we will use the length of the longest sentence as the standard and pad the other sentences with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max(text, key=len))\n",
    "\n",
    "for i in range(len(text)):\n",
    "    while len(text[i]) < maxlen: \n",
    "        text[i] += ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now since this is going to be a sequential predictive task, we would need to engineer the raw text data we have. \n",
    "#### We will have to divide the input data such that the last input character will be excluded and the target truth label be taken note as the 'correct answer' for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "      \n",
    "      # Remove first character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The target sequence will always be one-time step ahead of the input sequence.\n",
    "#### Now let us convert the sequence of characters to sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int) # This will dictate the size of our one-hot vector\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "\n",
    "# Making them tensors\n",
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that our data are in good shape, we will implement the model.\n",
    "#### For this model, we'll be using 1 layer of RNN followed by a fully connected layer for the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers \n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True) \n",
    "        self.fc = nn.Linear(hidden_dim, output_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0) \n",
    "\n",
    "        hidden = self.init__hidden(batch_size) \n",
    "\n",
    "        out, hidden = self.rnn(x,hidden) \n",
    "\n",
    "        out = out.contiguous().view(-1, self.hidden_dim) \n",
    "        out = self.fc(out) \n",
    "\n",
    "        return out, hidden \n",
    "\n",
    "    def init__hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device = device) \n",
    "        return hidden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's define our epochs and learning rate, and also decide on which device to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(dict_size, dict_size, 12, 1).to(device)\n",
    "\n",
    "n_epochs = 100 \n",
    "lr = 0.01 \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 2.4471\n",
      "Epoch: 20/100............. Loss: 2.2288\n",
      "Epoch: 30/100............. Loss: 1.9065\n",
      "Epoch: 40/100............. Loss: 1.4787\n",
      "Epoch: 50/100............. Loss: 1.0803\n",
      "Epoch: 60/100............. Loss: 0.7544\n",
      "Epoch: 70/100............. Loss: 0.5252\n",
      "Epoch: 80/100............. Loss: 0.3736\n",
      "Epoch: 90/100............. Loss: 0.2710\n",
      "Epoch: 100/100............. Loss: 0.2033\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    output, hidden = model(input_seq.to(device))\n",
    "    loss = criterion(output, target_seq.to(device).view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size:30px;\">Evaluation</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see what kind of output we will get with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good i am fine '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 15, 'good') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guess we're getting somewhere :) \n",
    "#### Of course the model has seen good before and it is obviously only accurate enough when we feed in words that it saw. \n",
    "#### If we feed in words that it never saw, it would probably throw back very inaccurate and awkward words to finish the sentence as the data we fed in was extremely limited in magnitude and scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample(model, \u001b[39m15\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mbreakfast\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m, in \u001b[0;36msample\u001b[0;34m(model, out_len, start)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Now pass in the previous characters and get a new one\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m ii \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(size):\n\u001b[0;32m---> 10\u001b[0m     char, h \u001b[39m=\u001b[39m predict(model, chars)\n\u001b[1;32m     11\u001b[0m     chars\u001b[39m.\u001b[39mappend(char)\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(chars)\n",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, character)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(model, character):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# One-hot encoding our input to fit into the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     character \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[char2int[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m character]])\n\u001b[1;32m      5\u001b[0m     character \u001b[39m=\u001b[39m one_hot_encode(character, dict_size, character\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m     character \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(character)\n",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(model, character):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# One-hot encoding our input to fit into the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     character \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[char2int[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m character]])\n\u001b[1;32m      5\u001b[0m     character \u001b[39m=\u001b[39m one_hot_encode(character, dict_size, character\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m     character \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(character)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b'"
     ]
    }
   ],
   "source": [
    "sample(model, 15, 'breakfast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, it won't even be able process the word as it does not have the letter b in its dictionary. \n",
    "#### This is where we can see the limitation of this current implementation and let me talk more about the limitations of RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size:30px;\">Model Limitations</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "#### With this data, it is limited in scope and magnitude which means it would only be respond similar to what it was fed in.\n",
    "#### If we started to ask about politics and not greetings, it would have no idea how to respond and would throw gibberish back at us.\n",
    "\n",
    "## Handling of unseen characters\n",
    "\n",
    "#### Additionally, as we saw, the model is unable to handle unseen characters.\n",
    "#### It needs to be able to one hot encode characters as vectors and not having seen an character, it will be unable to do so. \n",
    "\n",
    "## Representation of Textual Data \n",
    "\n",
    "#### In this notebook, we used one-hot encoding representation of textual data. \n",
    "#### However, this method is highly inefficient as it would mainly generate an extremely sparse matrix which would only contribute to inefficient space complexity. \n",
    "#### Additionally, it cannot contribute any contextual or semantic information with this representation. \n",
    "#### Modern NLP solutions rely on different word embeddings such as word2vec.\n",
    "#### These methods allow the model to learn meanings of word based on the context. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
