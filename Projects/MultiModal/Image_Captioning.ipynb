{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:04.835237Z","iopub.execute_input":"2023-06-11T16:14:04.835849Z","iopub.status.idle":"2023-06-11T16:14:15.907812Z","shell.execute_reply.started":"2023-06-11T16:14:04.835799Z","shell.execute_reply":"2023-06-11T16:14:15.906345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom matplotlib import image as mpimg\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torch.utils.data import Dataset as Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as tt\nfrom PIL import Image\nimport torchtext\nfrom torchtext.data import get_tokenizer\nfrom transformers import BertTokenizer\nfrom torchinfo import summary\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-11T16:14:15.910657Z","iopub.execute_input":"2023-06-11T16:14:15.911071Z","iopub.status.idle":"2023-06-11T16:14:15.921267Z","shell.execute_reply.started":"2023-06-11T16:14:15.911011Z","shell.execute_reply":"2023-06-11T16:14:15.920255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/flickr8k/captions.txt') as f:\n    contents = f.readlines()\ncontents = contents[1:]","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:15.922886Z","iopub.execute_input":"2023-06-11T16:14:15.923474Z","iopub.status.idle":"2023-06-11T16:14:15.947580Z","shell.execute_reply.started":"2023-06-11T16:14:15.923441Z","shell.execute_reply":"2023-06-11T16:14:15.946567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ntoken_ids = tokenizer.encode(\"You can now install TorchText using pip!\", padding=\"max_length\", max_length=20)\nback_to_words = tokenizer.convert_ids_to_tokens( token_ids ) ","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:15.952458Z","iopub.execute_input":"2023-06-11T16:14:15.952766Z","iopub.status.idle":"2023-06-11T16:14:16.150436Z","shell.execute_reply.started":"2023-06-11T16:14:15.952740Z","shell.execute_reply":"2023-06-11T16:14:16.149559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_name_list = []\ncaption_list = []\n\nfor line in contents:\n    image_name = line.split(',')[0]\n    caption = tokenizer.encode(\"\".join(line[:-1].split(',')[1:]), padding='max_length', max_length = 30, truncation=True)\n    image_name_list.append(image_name)\n    caption_list.append(caption)\n    \ncaption_df = pd.DataFrame({'Image_name': image_name_list, 'Caption': caption_list})","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:16.151886Z","iopub.execute_input":"2023-06-11T16:14:16.152320Z","iopub.status.idle":"2023-06-11T16:14:37.197367Z","shell.execute_reply.started":"2023-06-11T16:14:16.152286Z","shell.execute_reply":"2023-06-11T16:14:37.196446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_df.drop_duplicates(subset='Image_name', inplace=True)\ncaption_df","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.198904Z","iopub.execute_input":"2023-06-11T16:14:37.199375Z","iopub.status.idle":"2023-06-11T16:14:37.226168Z","shell.execute_reply.started":"2023-06-11T16:14:37.199341Z","shell.execute_reply":"2023-06-11T16:14:37.225346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = mpimg.imread(\"/kaggle/input/flickr8k/Images/2544246151_727427ee07.jpg\")\nplt.imshow(image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.227631Z","iopub.execute_input":"2023-06-11T16:14:37.228046Z","iopub.status.idle":"2023-06-11T16:14:37.608884Z","shell.execute_reply.started":"2023-06-11T16:14:37.228007Z","shell.execute_reply":"2023-06-11T16:14:37.607939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image(img, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    \n    #unnormalize \n    img[0] = img[0] * 0.229\n    img[1] = img[1] * 0.224 \n    img[2] = img[2] * 0.225 \n    img[0] += 0.485 \n    img[1] += 0.456 \n    img[2] += 0.406\n    \n    img = img.numpy().transpose((1, 2, 0))\n    \n    \n    plt.imshow(img)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.610192Z","iopub.execute_input":"2023-06-11T16:14:37.611214Z","iopub.status.idle":"2023-06-11T16:14:37.619269Z","shell.execute_reply.started":"2023-06-11T16:14:37.611179Z","shell.execute_reply":"2023-06-11T16:14:37.618127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data making","metadata":{}},{"cell_type":"code","source":"train_transform_trivial_augment = tt.Compose([\n    torchvision.models.ResNet101_Weights.IMAGENET1K_V2.transforms()\n])\n\ntest_transform = tt.Compose([\n    torchvision.models.ResNet101_Weights.IMAGENET1K_V2.transforms()\n])","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.620856Z","iopub.execute_input":"2023-06-11T16:14:37.621204Z","iopub.status.idle":"2023-06-11T16:14:37.631047Z","shell.execute_reply.started":"2023-06-11T16:14:37.621167Z","shell.execute_reply":"2023-06-11T16:14:37.629843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageToCaption(Dataset):\n    def __init__(self, df, transform):\n        self.data_dict = df\n        self.transform = transform\n        \n        labels = self.data_dict['Caption']\n        \n    def __len__(self):\n        return len(self.data_dict)\n    \n    def __getitem__(self, idx):\n        \n        img = self.data_dict['Image_name'].iloc[idx]\n        label = self.data_dict['Caption'].iloc[idx]\n        img = Image.open(\"/kaggle/input/flickr8k/Images/\" + img)\n            \n        return self.transform(img), torch.tensor(label)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.636967Z","iopub.execute_input":"2023-06-11T16:14:37.637275Z","iopub.status.idle":"2023-06-11T16:14:37.645583Z","shell.execute_reply.started":"2023-06-11T16:14:37.637252Z","shell.execute_reply":"2023-06-11T16:14:37.644680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = caption_df.iloc[:7000]\ntest_data = caption_df.iloc[7000:]","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.647187Z","iopub.execute_input":"2023-06-11T16:14:37.647538Z","iopub.status.idle":"2023-06-11T16:14:37.654645Z","shell.execute_reply.started":"2023-06-11T16:14:37.647508Z","shell.execute_reply":"2023-06-11T16:14:37.653637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_custom = ImageToCaption(train_data, transform = train_transform_trivial_augment)\ntest_data_custom = ImageToCaption(test_data, transform = test_transform)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.657141Z","iopub.execute_input":"2023-06-11T16:14:37.657882Z","iopub.status.idle":"2023-06-11T16:14:37.663696Z","shell.execute_reply.started":"2023-06-11T16:14:37.657824Z","shell.execute_reply":"2023-06-11T16:14:37.662594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\n\ntrain_dataloader_custom = DataLoader(dataset=train_data_custom, \n                                     batch_size=BATCH_SIZE, \n                                     # num_workers=1,\n                                     shuffle=True) \n\ntest_dataloader_custom = DataLoader(dataset=test_data_custom, \n                                    batch_size=BATCH_SIZE, \n                                    # num_workers=1, \n                                    shuffle=False) \n\ntrain_dataloader_custom, test_dataloader_custom","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.664859Z","iopub.execute_input":"2023-06-11T16:14:37.665151Z","iopub.status.idle":"2023-06-11T16:14:37.677028Z","shell.execute_reply.started":"2023-06-11T16:14:37.665128Z","shell.execute_reply":"2023-06-11T16:14:37.676075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_data, caption = next(iter(train_dataloader_custom))\n\nprint(f\"Image shape: {img_data.shape} -> [batch_size, color_channels, height, width]\\n\")\nprint(f\"Label shape: {caption.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:37.678528Z","iopub.execute_input":"2023-06-11T16:14:37.678919Z","iopub.status.idle":"2023-06-11T16:14:37.912474Z","shell.execute_reply.started":"2023-06-11T16:14:37.678884Z","shell.execute_reply":"2023-06-11T16:14:37.911409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = torchvision.models.resnet101(weights=torchvision.models.ResNet101_Weights.DEFAULT)\n\n        # Remove linear and pool layers (since we're not doing classification)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n\n        # Resize image to fixed size to allow input images of variable size\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        self.fine_tune()\n\n    def forward(self, images):\n        \"\"\"\n        Forward propagation.\n\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"\n        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n        out = out.view(out.size(0), -1, out.size(-1))\n        return out\n\n    def fine_tune(self, fine_tune=False):\n        \"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune\n                \nclass Attention(nn.Module):\n    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n        super(Attention, self).__init__()\n        \n        self.attention_dim = attention_dim\n        \n        self.W = nn.Linear(decoder_dim,attention_dim)\n        self.U = nn.Linear(encoder_dim,attention_dim)\n        \n        self.A = nn.Linear(attention_dim,1)\n        \n        self.relu = nn.ReLU()\n        \n        \n        \n        \n    def forward(self, features, hidden_state):\n        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n        \n        combined_states = self.relu(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n        \n        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n        \n        \n        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n        \n        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n        \n        return alpha,attention_weights\n        \nclass DecoderRNN(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        \n        #save the model param\n        self.vocab_size = vocab_size\n        self.attention_dim = attention_dim\n        self.decoder_dim = decoder_dim\n        \n        self.embedding = nn.Embedding(vocab_size,embed_size)\n        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n        \n        \n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        \n        \n        self.fcn = nn.Linear(decoder_dim,vocab_size)\n        self.drop = nn.Dropout(drop_prob)\n        \n        \n    \n    def forward(self, features, captions):\n        \n        #vectorize the caption\n        embeds = self.embedding(captions)\n        \n        # Initialize LSTM state\n        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n        \n        #get the seq length to iterate\n        seq_length = len(captions[0])-1 #Exclude the last one\n        batch_size = captions.size(0)\n        num_features = features.size(1)\n        \n        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n                \n        for s in range(seq_length):\n            alpha,context = self.attention(features, h)\n            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n                    \n            output = self.fcn(self.drop(h))\n            \n            preds[:,s] = output\n            alphas[:,s] = alpha  \n        \n        \n        return preds, alphas\n    \n    def generate_caption(self,features,max_len=20,tokenizer = tokenizer):\n        # Inference part\n        # Given the image features generate the captions\n        \n        batch_size = features.size(0)\n        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n        \n        alphas = []\n        \n        #starting input\n        word = torch.tensor(tokenizer.vocab['[CLS]']).view(1,-1).to(device)\n        embeds = self.embedding(word)\n\n        \n        captions = []\n        \n        for i in range(max_len):\n            alpha,context = self.attention(features, h)\n            \n            \n            #store the apla score\n            alphas.append(alpha.cpu().detach().numpy())\n            \n            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n            output = self.fcn(self.drop(h))\n            output = output.view(batch_size,-1)\n        \n            \n            #select the word with most val\n            predicted_word_idx = output.argmax(dim=1)\n            \n            #save the generated word\n            captions.append(predicted_word_idx.item())\n            \n            #end if <EOS detected>\n            if tokenizer.convert_ids_to_tokens( predicted_word_idx.item() ) == \"[SEP]\":\n                break\n            \n            #send generated word as the next caption\n            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n        \n        #covert the vocab idx to words and return sentence\n        return [tokenizer.convert_ids_to_tokens( idx ) for idx in captions],alphas\n    \n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n    \nclass EncoderDecoder(nn.Module):\n    def __init__(self,embed_size=300, vocab_size=30_000, attention_dim = 256,encoder_dim=2048,decoder_dim=512,drop_prob=0.3):\n        super().__init__()\n        self.encoder = Encoder()\n        self.decoder = DecoderRNN(\n            embed_size=embed_size,\n            vocab_size = vocab_size,\n            attention_dim=attention_dim,\n            encoder_dim=encoder_dim,\n            decoder_dim=decoder_dim\n        )\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:15:28.891131Z","iopub.execute_input":"2023-06-11T17:15:28.891628Z","iopub.status.idle":"2023-06-11T17:15:28.924961Z","shell.execute_reply.started":"2023-06-11T17:15:28.891592Z","shell.execute_reply":"2023-06-11T17:15:28.923945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"device = 'cuda'\nlr = 0.0004\nembed_size = 512\nvocab_size = 30_000\nattention_dim = 512\nencoder_dim = 2048 \ndecoder_dim = 512\ndrop_prob = 0.5\nepochs = 30\noutput_interval = 50\nalpha_c = 1.\ngrad_clip = 5.\n\nmodel = EncoderDecoder(embed_size, vocab_size, attention_dim, encoder_dim, decoder_dim, drop_prob).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.vocab['[PAD]']).to(device)\noptimizer = optim.RMSprop(model.parameters(), lr=lr)\nmodel.to(device)\nsummary(model)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:55:54.854953Z","iopub.execute_input":"2023-06-11T17:55:54.855326Z","iopub.status.idle":"2023-06-11T17:55:56.135214Z","shell.execute_reply.started":"2023-06-11T17:55:54.855291Z","shell.execute_reply":"2023-06-11T17:55:56.134285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataiter = iter(test_dataloader_custom)\n\nfor epoch in tqdm(range(1,epochs+1)):\n    for train_idx, (image, caption) in enumerate(iter(train_dataloader_custom)):\n        image, caption = image.to(device), caption.to(device)\n        optimizer.zero_grad()\n        outputs,attentions = model(image, caption)\n        \n        targets = caption[:, 1:]\n        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n        loss += alpha_c * ((1. - attentions.sum(dim=1)) ** 2).mean() # mentioned in the paper\n        loss.backward() \n        \n        for group in optimizer.param_groups: # Gradient Clipping \n            for param in group['params']:\n                if param.grad is not None:\n                    param.grad.data.clamp_(-grad_clip, grad_clip)\n        \n        optimizer.step()\n        \n        if (train_idx + 1) % output_interval == 0:\n            \n            #generate the caption\n            model.eval()\n            with torch.no_grad():\n                val_loss = 0\n                count = 0\n                for idx, (image,caption) in enumerate(iter(test_dataloader_custom)):\n                    image, caption = image.to(device), caption.to(device)\n                    outputs, attentions = model(image, caption)\n                    targets = caption[:, 1:]\n                    val_loss += (criterion(outputs.view(-1, vocab_size), targets.reshape(-1)).item() + alpha_c * ((1. - attentions.sum(dim=1)) ** 2).mean())\n                    count += 1\n                val_loss /= count\n            \n            print(f\"Epoch: {epoch} | Index: {train_idx+1} | Training Loss : {loss.item()} | Validation Loss : {val_loss}\")\n            model.train()\n                \n    img,_ = next(dataiter)\n    features = model.encoder(img[0:1].to(device))\n    caps,alphas = model.decoder.generate_caption(features, max_len = 30, tokenizer = tokenizer)\n    caption = ' '.join(caps)\n    show_image(img[0],title=caption)               \n            ","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:55:56.137073Z","iopub.execute_input":"2023-06-11T17:55:56.137745Z","iopub.status.idle":"2023-06-11T18:06:33.838695Z","shell.execute_reply.started":"2023-06-11T17:55:56.137704Z","shell.execute_reply":"2023-06-11T18:06:33.837186Z"},"trusted":true},"execution_count":null,"outputs":[]}]}