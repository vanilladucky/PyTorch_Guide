{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size:50px;\">Word2Vec</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook, I will be attempting to implement a simple word2vec on the wikitext dataset. \n",
    "#### By vectorizing and embedding words in a certain dimensional vector, I hope to be able to capture word's meanings and context. \n",
    "#### This specific technique is relatively outdated but the whole field of vectorizing words and embedding them to a certain dimensional vector is paramount to any Language Processing tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/kimhyunbin/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cb028b9c1b41ba831292e37462babc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import gc\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn \n",
    "embedding_dim = 300 \n",
    "norm = 1 # max_norm is currently not supported on the mps backend\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features = vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building up a vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator \n",
    "MIN_WORD_FREQUENCY = 50\n",
    "\n",
    "def build_vocab(data_iter, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        map(tokenizer, data_iter),\n",
    "        specials=[\"<unk>\"],\n",
    "        min_freq=MIN_WORD_FREQUENCY,\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1177"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = dataset['train']['text']\n",
    "val_set = dataset['validation']['text']\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "vocab = build_vocab(train_set, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 4099 words in our vocab\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(vocab)} words in our vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making our custom DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "CBOW_N_WORDS = 4 \n",
    "MAX_SEQUENCE_LENGTH = 256  \n",
    "def collate_cbow(batch, text_pipeline):\n",
    "     batch_input, batch_output = [], []\n",
    "     for text in batch:\n",
    "         text_tokens_ids = text_pipeline(text)\n",
    "         if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "             continue\n",
    "         if MAX_SEQUENCE_LENGTH:\n",
    "             text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "         for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "             token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]\n",
    "             output = token_id_sequence.pop(CBOW_N_WORDS)\n",
    "             input_ = token_id_sequence\n",
    "             batch_input.append(input_)\n",
    "             batch_output.append(output)\n",
    "     \n",
    "     batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "     batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "     return batch_input, batch_output\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "from torch.utils.data import DataLoader \n",
    "from functools import partial  \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "         train_set,\n",
    "         batch_size=32,\n",
    "         shuffle=True,         \n",
    "         collate_fn=partial(collate_cbow, text_pipeline=text_pipeline),\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "         val_set,\n",
    "         batch_size=32,\n",
    "         shuffle=False,         \n",
    "         collate_fn=partial(collate_cbow, text_pipeline=text_pipeline),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Word2Vec                                 --\n",
       "├─Embedding: 1-1                         1,229,700\n",
       "├─Linear: 1-2                            1,233,799\n",
       "=================================================================\n",
       "Total params: 2,463,499\n",
       "Trainable params: 2,463,499\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device('mps')\n",
    "print(f\"Device set to {device}\")\n",
    "\n",
    "model = Word2Vec(len(vocab)).to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Epoch 0====\n",
      "Batch: 100 | Training Loss: 5.700555171966553\n",
      "Batch: 200 | Training Loss: 5.460875282287597\n",
      "Batch: 300 | Training Loss: 5.327524420420329\n",
      "Batch: 400 | Training Loss: 5.246071569919586\n",
      "Batch: 500 | Training Loss: 5.195186476707459\n",
      "Batch: 600 | Training Loss: 5.154842410087586\n",
      "Batch: 700 | Training Loss: 5.12525526864188\n",
      "Batch: 800 | Training Loss: 5.096050428152084\n",
      "Batch: 900 | Training Loss: 5.0706856139500935\n",
      "Batch: 1000 | Training Loss: 5.050419266700745\n",
      "Batch: 1100 | Training Loss: 5.033205549933694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:09<19:23, 129.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.025709538509622 | Validation Loss: 4.848667940850985\n",
      "====Epoch 1====\n",
      "Batch: 100 | Training Loss: 4.43030499458313\n",
      "Batch: 200 | Training Loss: 4.46770458817482\n",
      "Batch: 300 | Training Loss: 4.501820142269135\n",
      "Batch: 400 | Training Loss: 4.521402886509895\n",
      "Batch: 500 | Training Loss: 4.535906949520111\n",
      "Batch: 600 | Training Loss: 4.55255509018898\n",
      "Batch: 700 | Training Loss: 4.566694319929395\n",
      "Batch: 800 | Training Loss: 4.576878043115139\n",
      "Batch: 900 | Training Loss: 4.590406875875261\n",
      "Batch: 1000 | Training Loss: 4.598806721925736\n",
      "Batch: 1100 | Training Loss: 4.606646579178896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:17<17:10, 128.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.610378376490563 | Validation Loss: 4.8840721825421864\n",
      "====Epoch 2====\n",
      "Batch: 100 | Training Loss: 4.245000936985016\n",
      "Batch: 200 | Training Loss: 4.267647408246994\n",
      "Batch: 300 | Training Loss: 4.3123674734433495\n",
      "Batch: 400 | Training Loss: 4.3421886259317395\n",
      "Batch: 500 | Training Loss: 4.37359833574295\n",
      "Batch: 600 | Training Loss: 4.3952649104595185\n",
      "Batch: 700 | Training Loss: 4.415440137386322\n",
      "Batch: 800 | Training Loss: 4.435311916768551\n",
      "Batch: 900 | Training Loss: 4.450406462086572\n",
      "Batch: 1000 | Training Loss: 4.461797515153885\n",
      "Batch: 1100 | Training Loss: 4.475394317670302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [06:31<15:16, 130.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.481946669181465 | Validation Loss: 4.907598487401413\n",
      "====Epoch 3====\n",
      "Batch: 100 | Training Loss: 4.106807301044464\n",
      "Batch: 200 | Training Loss: 4.170511000156402\n",
      "Batch: 300 | Training Loss: 4.220629930496216\n",
      "Batch: 400 | Training Loss: 4.250997334718704\n",
      "Batch: 500 | Training Loss: 4.285760184288025\n",
      "Batch: 600 | Training Loss: 4.308191334406535\n",
      "Batch: 700 | Training Loss: 4.329201340675354\n",
      "Batch: 800 | Training Loss: 4.3495030874013905\n",
      "Batch: 900 | Training Loss: 4.363507532013787\n",
      "Batch: 1000 | Training Loss: 4.376463479042053\n",
      "Batch: 1100 | Training Loss: 4.3924466566606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:37<12:54, 129.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.398389753979673 | Validation Loss: 4.922367108070244\n",
      "====Epoch 4====\n",
      "Batch: 100 | Training Loss: 4.0406603074073795\n",
      "Batch: 200 | Training Loss: 4.104192316532135\n",
      "Batch: 300 | Training Loss: 4.14416809240977\n",
      "Batch: 400 | Training Loss: 4.1739281618595125\n",
      "Batch: 500 | Training Loss: 4.209615782737732\n",
      "Batch: 600 | Training Loss: 4.235032593011856\n",
      "Batch: 700 | Training Loss: 4.256171023845672\n",
      "Batch: 800 | Training Loss: 4.27484176069498\n",
      "Batch: 900 | Training Loss: 4.289953530364566\n",
      "Batch: 1000 | Training Loss: 4.304309509038925\n",
      "Batch: 1100 | Training Loss: 4.317463500499725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [10:27<10:10, 122.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.32297665669943 | Validation Loss: 4.947966721098302\n",
      "====Epoch 5====\n",
      "Batch: 100 | Training Loss: 3.993021776676178\n",
      "Batch: 200 | Training Loss: 4.038391724824906\n",
      "Batch: 300 | Training Loss: 4.073778836727143\n",
      "Batch: 400 | Training Loss: 4.110183302760124\n",
      "Batch: 500 | Training Loss: 4.139734154224396\n",
      "Batch: 600 | Training Loss: 4.168943432569503\n",
      "Batch: 700 | Training Loss: 4.190500344889505\n",
      "Batch: 800 | Training Loss: 4.212404606044292\n",
      "Batch: 900 | Training Loss: 4.227655019495223\n",
      "Batch: 1000 | Training Loss: 4.242129618406296\n",
      "Batch: 1100 | Training Loss: 4.256721100156957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [12:18<07:53, 118.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.261824580227456 | Validation Loss: 4.968322523569657\n",
      "====Epoch 6====\n",
      "Batch: 100 | Training Loss: 3.93473486661911\n",
      "Batch: 200 | Training Loss: 3.982893364429474\n",
      "Batch: 300 | Training Loss: 4.022195676962535\n",
      "Batch: 400 | Training Loss: 4.051426775455475\n",
      "Batch: 500 | Training Loss: 4.082562015533448\n",
      "Batch: 600 | Training Loss: 4.111786295572917\n",
      "Batch: 700 | Training Loss: 4.135510178293501\n",
      "Batch: 800 | Training Loss: 4.153130821287632\n",
      "Batch: 900 | Training Loss: 4.174414698547787\n",
      "Batch: 1000 | Training Loss: 4.190364904642105\n",
      "Batch: 1100 | Training Loss: 4.204964989965612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [14:00<05:38, 112.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.209700869143217 | Validation Loss: 4.976158081474951\n",
      "====Epoch 7====\n",
      "Batch: 100 | Training Loss: 3.8992581605911254\n",
      "Batch: 200 | Training Loss: 3.938395303487778\n",
      "Batch: 300 | Training Loss: 3.981775953769684\n",
      "Batch: 400 | Training Loss: 4.015457997918129\n",
      "Batch: 500 | Training Loss: 4.045438268661499\n",
      "Batch: 600 | Training Loss: 4.069670852820079\n",
      "Batch: 700 | Training Loss: 4.088676358972277\n",
      "Batch: 800 | Training Loss: 4.106462588906288\n",
      "Batch: 900 | Training Loss: 4.12276763147778\n",
      "Batch: 1000 | Training Loss: 4.13587255358696\n",
      "Batch: 1100 | Training Loss: 4.150331523418426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [15:42<03:39, 109.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.154829735747613 | Validation Loss: 4.994739083920495\n",
      "====Epoch 8====\n",
      "Batch: 100 | Training Loss: 3.8436225986480714\n",
      "Batch: 200 | Training Loss: 3.893544706106186\n",
      "Batch: 300 | Training Loss: 3.93956129471461\n",
      "Batch: 400 | Training Loss: 3.9707255399227144\n",
      "Batch: 500 | Training Loss: 3.992917022705078\n",
      "Batch: 600 | Training Loss: 4.016055106719335\n",
      "Batch: 700 | Training Loss: 4.035325904573713\n",
      "Batch: 800 | Training Loss: 4.053538103401661\n",
      "Batch: 900 | Training Loss: 4.071452567577362\n",
      "Batch: 1000 | Training Loss: 4.08826977467537\n",
      "Batch: 1100 | Training Loss: 4.102739448764107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [17:17<01:45, 105.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.1097867250027145 | Validation Loss: 5.028427144228401\n",
      "====Epoch 9====\n",
      "Batch: 100 | Training Loss: 3.8041112923622133\n",
      "Batch: 200 | Training Loss: 3.8516740250587462\n",
      "Batch: 300 | Training Loss: 3.8871687014897662\n",
      "Batch: 400 | Training Loss: 3.9212903106212615\n",
      "Batch: 500 | Training Loss: 3.9474253516197204\n",
      "Batch: 600 | Training Loss: 3.9703133726119995\n",
      "Batch: 700 | Training Loss: 3.9924007661002023\n",
      "Batch: 800 | Training Loss: 4.014044698178768\n",
      "Batch: 900 | Training Loss: 4.033193284935422\n",
      "Batch: 1000 | Training Loss: 4.049390785694122\n",
      "Batch: 1100 | Training Loss: 4.0628356998617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [18:48<00:00, 112.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.069798013682149 | Validation Loss: 5.048944719767166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lr = 0.025\n",
    "epochs = 10\n",
    "log_interval = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"====Epoch {epoch}====\")\n",
    "    model.train()\n",
    "    train_loss, train_count = 0, 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_count += 1\n",
    "\n",
    "        if (batch+1) % log_interval == 0:\n",
    "            print(f\"Batch: {batch+1} | Training Loss: {train_loss/train_count}\")\n",
    "\n",
    "    # Validation \n",
    "    model.eval()\n",
    "    val_loss, val_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(val_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "            val_count += 1 \n",
    "    print(f\"Training Loss: {train_loss/train_count} | Validation Loss: {val_loss/val_count}\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the embeddings and relationships between different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# embedding from first model layer\n",
    "embeddings = list(model.parameters())[0]\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# normalization\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(word, n = 5):\n",
    "    idx = vocab[word]\n",
    "    if idx == 0:\n",
    "        print(\"Out of vocab word\")\n",
    "        return \n",
    "\n",
    "    word_embed = embeddings_norm[idx]\n",
    "    distance = np.matmul(embeddings_norm, word_embed.reshape(-1,1)).flatten()\n",
    "    descending_order = np.argsort(-1 * distance)[1: n+1]\n",
    "\n",
    "    top_dict = {}\n",
    "    for sim_word_id in descending_order:\n",
    "        sim_word = vocab.lookup_token(sim_word_id)\n",
    "        top_dict[sim_word] = distance[sim_word_id]\n",
    "    return top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desire: 0.239\n",
      "jane: 0.236\n",
      "richard: 0.235\n",
      "house: 0.227\n",
      "battle: 0.227\n"
     ]
    }
   ],
   "source": [
    "for word, sim in get_top_words(\"queen\").items():\n",
    "    if word == None:\n",
    "        print(\"Out of vocab\")\n",
    "    else:\n",
    "        print(\"{}: {:.3f}\".format(word, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us try the famous King - Man + Woman = Queen problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_embeddings = embeddings_norm[vocab['king']] - embeddings_norm[vocab['man']] + embeddings_norm[vocab['woman']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_from_embedding(embedding, n = 5):\n",
    "\n",
    "    distance = np.matmul(embeddings_norm, embedding.reshape(-1,1)).flatten()\n",
    "    descending_order = np.argsort(-1 * distance)[1: n+1]\n",
    "\n",
    "    top_dict = {}\n",
    "    for sim_word_id in descending_order:\n",
    "        sim_word = vocab.lookup_token(sim_word_id)\n",
    "        top_dict[sim_word] = distance[sim_word_id]\n",
    "    return top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman: 0.755\n",
      "prerogative: 0.386\n",
      "goddess: 0.385\n",
      "patrick: 0.359\n",
      "church: 0.326\n"
     ]
    }
   ],
   "source": [
    "for word, sim in get_top_words_from_embedding(unknown_embeddings).items():\n",
    "    if word == None:\n",
    "        print(\"Out of vocab\")\n",
    "    else:\n",
    "        print(\"{}: {:.3f}\".format(word, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sadly, we didn't get the Queen that we were looking for. \n",
    "### Given that we only trained on a small dataset, it seems pretty reasonable. \n",
    "### With a bigger dataset and a more complex model architecture, I believe we can accomplish that task. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('machinelearning': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68339890f90b7cc04351217391b998bebccc50e4c246697aa46d0c3bd6df040a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
